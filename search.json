[{"title":"从当下开始","url":"/2025/08/23/%E4%BB%8E%E5%BD%93%E4%B8%8B%E5%BC%80%E5%A7%8B/","content":"对学历的理解本硕双非，曾经的理解，双非就像一个标签刻在自己身上，也是会有一种想法，觉得考上双非特别是研究生，是不是落后别人一大截，怎么挺身而出？其实在现在看来，我不会这么想了，每个人都会有自己的一段路程，在读研的过程中，我深刻体会到需要自洽、自强，好的学历会接触更多优秀的人、有更好的资源，强大的开始就是对学历祛魅跟自己和解，找到自己的舒服方式，接触更多优秀的人，让自己有实力有能力去接受挑战，双非不是标签，理性看待学历，专注于自身硬实力和软实力的成长。\n对自律的理解最近在寻找 AI 相关资料时，看到了一个博主的技术博客，特别是博主写的碎碎念。我开始深思：自己在过去的一段时间里，好像不太明白学习的目的究竟是什么，没有明确坚定的方向，学习似乎更多是为了感动自己。我开始思考何为自律，是自愿的学习，还是仅仅担忧未来是否能够有一个好的事业。回头想想，我没有坚持培养积极的爱好，容易陷入误区。\n我也看到许多高考励志的故事，或许在今后的路上，我再也不会指责过去的自己。无论结果如何，如果可以再来一次，我可能会走上相似的道路，经历相似的挑战。\n关于学习与成长我很喜欢阅读美文，自己表达的能力并不强，常常遇到一些好的文章或句子时，会将它们收藏起来。在业余时间，我会选择去阅读书籍，这是之前的领导给我的建议，很多事情其实可以在书中找到答案。虽然一些道理我已经看过很多遍，但真正能执行下去的却并不多。\n从这么长时间的读书生涯中，我意识到自己并不是一个特别擅长学习的人，对自己的专业也没有特别深的热爱。有时候，学习并没有深入专研，我一直处于输入阶段，却没有足够的输出。与此同时，我也意识到软实力同样重要——交际表达、为人处事等等。\n未来的路在今后的日子里，可能我会走得比较慢，但我依然希望不负当下的时光，成为一个更好的自己。我会向优秀的人学习，跳出眼前的视野，继续前行。\n","categories":["碎碎念念"],"tags":["碎碎念念"]},{"title":"LangChain","url":"/2025/08/25/LangChain/","content":"\n本篇文章主要介绍LangChain基础语法，RAG原理，以及使用LangChain实现简易的RAG系统，详细的RAG优化以及类型在另外的文章进行介绍\n\n1.LangChain 简介Langchains是用于开发大模型驱动的框架，借助LangChain可以轻松搭建基于LLM的相关应用，比如RAG系统\n此框架包含以下部分：\nlangchain-core: 基础抽象和LangChain表达式 (LCEL)langchain-community: 第三方集成，第三方相关的包langchain: 组成应用程序认知架构的链、代理和检索策略。LangGraph: 通过将步骤建模为图中的边和节点，构建强大且有状态的多参与者应用程序。与LangChain无缝集成，但也可以单独使用LangServe: 将LangChain链部署为REST APILangSmith: 一个开发者平台，让您调试、测试、评估和监控LLM应用程序\n\n\n\n\n\n\n\n2.LangChain 基础\nLangChain是为大模型应用而生的框架，结合大模型应用逻辑上，分为输入、模型处理、输出三个环节。通过构建提示词工程，作为上下文给大模型，然后通过结果解析器解析相应格式的输出\n\nLCEL表达式基于LangChain框架编写大模型应用的过程就像垒积木，其中的积木就是Prompts，LLMs和各OutputParser等。如何将这些积木组织起来，除了使用基本Python语法调用对应类的方法，一种更灵活的方法就是使用位于LangChain-Core层中的LCEL（LangChain Expression Language）(使用管道符来进行拼接)\nRunnable接口LCEL的基础是Runnable接口。通过实现Runnable接口，LCEL定义了一组具有通用调用方式的方法集。\nRunnable有一系列的子类，比如PromptTemplate、LLM和StrOutputParser（还有更多），这些组件子类都间接继承自Runnable(继承自RunnableSequence，而后者又继承自Runnable) 通过 管道符”|”前后两个组件合成一个Runnable的子孙类RunnableSequence对象返回，从而达到串起来形成链(chain)的目的。\n\nRunnable的__or__()方法重新定义了”|”语法，所以基于LCEL的chain就能通过或(也类似shell中的管道)操作符号”|”串起来。这也就是前面提到的“chain &#x3D; prompt | llm | output_parser”这行代码虽然看上去跟普通Python不一样，但它是合法的，原因就在这里\n\n3.LangChain实现RAG\nRAG的出现是为了在一定程度上缓解大模型的幻觉问题，因为大模型的输出是基于数学概率的输出预测、且信息不具有实效性、缺乏相关领域的专业知识，下面使用LangChain的链式调用实现简单的RAG。\n\n基础RAG系统，流程如下：\n\n文档切分\n向量嵌入\n构建提示词\n定义大模型服务\n检索相关片段作为大模型上下文\n结果解析器对大模型结果进行解析\n\n\n\n#完整的rag  from langchain_core.output_parsers  import StrOutputParserquery = &quot;在线支付取消订单后钱怎么返还&quot;from langchain_community.embeddings import DashScopeEmbeddingsfrom config.load_key import load_keyif not os.environ.get(&quot;DASHSCOPE_API_KEY&quot;):    os.environ[&quot;DASHSCOPE_API_KEY&quot;] = load_key(&quot;LANGSMITH_API_KEY&quot;)embedding_model = DashScopeEmbeddings(model=&quot;text-embedding-v1&quot;)redis_url = &quot;redis://localhost:6379&quot;config = RedisConfig(    index_name=&quot;meituan-index&quot;,   # 索引名，对应之前构建的知识库索引    redis_url=redis_url,          # Redis 连接地址)vector_store = RedisVectorStore(embedding_model, config=config)retriever = vector_store.as_retriever()#定义大模型from langchain_openai import  ChatOpenAIllm = ChatOpenAI(    model = &quot;deepseek-v3&quot;,    base_url =&quot;https://dashscope.aliyuncs.com/compatible-mode/v1 &quot;,    openai_key = load_key(&quot;LANGSMITH_API_KEY&quot;),) #定义提示模板from langchain_core.prompts import  ChatPromptTemplateprompt_template = ChatPromptTemplate.from_messages([    (&quot;user&quot;, &quot;你是一个答疑机器人，你的任务是根据下述给定的已知信息回答用户问题。\\n&quot;             &quot;已知信息：&#123;context&#125;\\n&quot;             &quot;用户问题：&#123;question&#125;\\n\\n&quot;             &quot;如果已知信息不含用户问题的答案，或者已知信息不足以回答用户问题，请直接回复“我无法回答您问题”。\\n&quot;             &quot;请不要输出已知信息中不包含的信息或答案。\\n&quot;             &quot;请用中文回答用户问题。&quot;)  ])#收集document内容def collect_document_content(segments):    text = []    for segment in segments:        text.append(segment.page_content)    return textfrom operator import itemgetterchain = (&#123;     &quot;question&quot;: query,    &quot;context&quot;: itemgetter(&quot;question&quot;)|retriever |collect_document_content,&#125;| prompt_template| llm| StrOutputParser())response = chain.invoke(&#123;&quot;question&quot;: query&#125;)  \n\n4.相关资料大模型应用开发学习资料\nLangChain中文文档\nCSDN博客\n","categories":["Langchain"],"tags":["AI"]},{"title":"transformer学习","url":"/2025/08/23/transformer%E5%AD%A6%E4%B9%A0/","content":"Transformer1. Transformer 主要结构及对比 RNN编码器（Encoder）编码器由多个相同的编码器层堆叠而成，每一层包含以下部分：\n\n多头自注意力机制（Multi-Head Self-Attention）：通过多个注意力头从不同角度学习词与词之间的关系。\n前馈神经网络（Feed-Forward Neural Networks）：对注意力机制的输出进行非线性变换，增强模型的表达能力。\n残差连接和层归一化（Residual Connection &amp; Layer Normalization）：帮助信息流动并稳定训练过程，防止梯度消失。\n\n解码器（Decoder）解码器与编码器类似，但每层解码器额外包含以下机制：\n\n掩蔽多头自注意力机制（Masked Multi-Head Attention）：用于处理目标序列，通过掩码防止当前位置关注未来位置，确保生成过程的自回归特性。\n\n编码器 - 解码器注意力机制（Encoder-Decoder Attention）：使解码器能够关注编码器输出的上下文信息，建立输入与输出序列之间的关联。\n\n前馈神经网络（Feed-Forward Neural Network）：对注意力机制的输出进行非线性变换，增强解码器的表达能力。\n\n\n\n\n对比 RNN\n优点：\n并行化：取消递归结构，Transformer 允许序列中的所有位置同时处理，而 RNN 是逐步处理的，这使得 Transformer 能在训练过程中实现并行化，从而大幅加速训练。\n长距离依赖：RNN 在处理长序列时，容易出现梯度消失或梯度爆炸的问题，而 Transformer 通过自注意力机制能够直接捕捉到序列中任意位置的依赖关系。（引入位置编码， 在不依赖 RNN 结构的情况下，通过位置编码为序列中的每个元素嵌入位置信息，从而使模型能够感知输入的顺序）\n计算效率：Transformer 在每一层的计算是对称的，计算效率较高，且可以扩展到大规模的训练数据。\n\n\nRNN 的缺点：\n训练速度慢：由于其顺序处理的特点，RNN 的计算速度较慢，无法进行高效并行。\n长程依赖问题：RNN 在捕捉长期依赖时表现较差，通常会遭遇梯度消失或爆炸问题，导致学习效果不佳。\n\n\n\n2. 为什么要进行位置嵌入（Positional Encoding）？位置嵌入的作用Transformer 不像 RNN 那样天然具有序列顺序的处理能力，因此需要通过位置嵌入来为模型提供位置信息，使其能理解输入数据的顺序。\n位置嵌入的方式最常用的方式是通过 正弦和余弦函数 来生成位置嵌入。具体做法是为每个位置计算一个向量，使用不同频率的正弦和余弦函数来表示不同位置的信息。这个方式的优点是它可以在不依赖训练的情况下生成，且具有很好的可扩展性。\ntransformer 的自注意力机制（Self-Attention）是位置无关（position-agnostic）的。也就是说，如果不做任何处理，模型无法区分 “我爱你” 和 “你爱我” 这两个句子的差异，因为自注意力机制只关注 token 之间的相关性，而不考虑它们在序列中的顺序。\n为了让模型感知到 token 的位置信息，Transformer 引入了位置编码。\n在原始论文中，Transformer 使用的是固定位置编码（Positional Encoding），其公式如下：\n\n\n其中：\npos 表示位置索引（Position）。\ni 表示维度索引。\ndmodel 是嵌入向量的维度。\n流程：输入的是一个整数索引（位置序号 0,1,2,…）。位置编码模块先把这些整数映射成与词向量同维度的向量（例如 512 维），再把结果加到词向量上。\n如何理解位置嵌入\nhttps://www.zhihu.com/question/347678607\n3. 自注意力机制自注意力机制的作用\n随着模型处理输入序列的每个单词，自注意力会关注整个输入序列的所有单词，帮助模型对本单词更好地进行编码。在处理过程中，自注意力机制会将对所有相关单词的理解融入到我们正在处理的单词中。更具体的功能如下：\n序列建模：自注意力可以用于序列数据（例如文本、时间序列、音频等）的建模。它可以捕捉序列中不同位置的依赖关系，从而更好地理解上下文。这对于机器翻译、文本生成、情感分析等任务非常有用。\n并行计算：自注意力可以并行计算，这意味着可以有效地在现代硬件上进行加速。相比于RNN和CNN等序列模型，它更容易在GPU和TPU等硬件上进行高效的训练和推理。（因为在自注意力中可以并行的计算得分）\n长距离依赖捕捉：传统的循环神经网络（RNN）在处理长序列时可能面临梯度消失或梯度爆炸的问题。自注意力可以更好地处理长距离依赖关系，因为它不需要按顺序处理输入序列。\n\nQ, K, V 的含义\nQ（Query）：查询向量，表示当前关注的词。\nK（Key）：键向量，表示其它词的特征。\nV（Value）：值向量，表示与键向量关联的实际信息。\n\n注意力分数计算公式\n\n\n\n注意力分数的计算方式是通过 Q 和 K 的点积 来衡量 Query 与 Key 之间的相关性。计算公式为：其中，d_k 是 K 向量的维度，* 表示矩阵乘法，softmax 用于标准化分数，使其成为概率分布。\n问题：为什么要除以dk？\n当 dk 较大时，点积的数值可能会过大，导致 Softmax 过后的梯度变得极小\n\n\n\n\n\n\n\n\n\n\nSoftmax函数\n\n在 Transformer 模型中，Softmax 函数不仅在计算注意力权重时用到，在预测阶段的输出处理环节也会用到，因为预测 token 的过程可以看成是多分类问题。\nSoftmax或称归一化指数函数，它将每一个元素的范围都压缩到（0，1）之间，并且所有元素的和为1\n最后经过线性层后进入softmax函数，将每个值向量乘以softmax分数(这是为了准备之后将它们求和)。这里的直觉是希望关注语义上相关的单词，并弱化不相关的单词(例如，让它们乘以0.001这样的小数)。\nSoftmax 通过指数变换放大数值间的差异，让较大的值对应更高的概率，同时避免了负值和数值过小的问题，让模型聚焦于权重最高的位置，同时保留全局信息（低权重仍非零）\n4. 多头注意力机制多头注意力机制就是存在多个不同的权重矩阵（Q、K、V），形成多个矩阵 Z，再把它们 按最后一维（hidden）拼接（concat）→ 做一次线性变换 得到最终输出。\n线性层把拼接后的多头结果 Z_concat（形状 batch×seq×d_model）重新线性映射回与输入相同的维度，同时让网络可以学习如何融合不同头的信息。\n\n\n\n\n\n\n\n\n\n\n5. 残差层和归一化层的作用Add（残差连接，Residual Connection）残差连接是一种跳跃连接（Skip Connection），它将层的输入直接加到输出上（观察架构图中的箭头）：\nAdd，就是在z的基础上加了一个残差块X，加入残差块的目的是为了防止在深度神经网络的训练过程中发生退化的问题，退化的意思就是深度神经网络通过增加网络的层数，Loss逐渐减小，然后趋于稳定达到饱和，然后再继续增加网络层数，Loss反而增大\n\n\n通过直接将输入添加到输出中，帮助缓解深度网络中的梯度消失问题，保证信息能够有效流动。这种连接方式有效缓解了深层神经网络的梯度消失问题\nResNet残差神经网络为了了解残差块，我们引入ResNet残差神经网络，神经网络退化指的是在达到最优网络层数之后，神经网络还在继续训练导致Loss增大，对于多余的层，我们需要保证多出来的网络进行恒等映射。只有进行了恒等映射之后才能保证这多出来的神经网络不会影响到模型的效果。残差连接主要是为了防止网络退化。\n\n\n上图就是构造的一个残差块，X是输入值，F（X）是经过第一层线性变换后并且激活的输出，在第二层线性变化之后，激活之前，F（X）加入了这一层输入值X，然后再进行激活后输出。\n要恒等映射，我们只需要让F（X）&#x3D;0就可以了。x经过线性变换（随机初始化权重一般偏向于0），输出值明显会偏向于0，而且经过激活函数Relu会将负数变为0，过滤了负数的影响。这样当网络自己决定哪些网络层为冗余层时，使用ResNet的网络很大程度上解决了学习恒等映射的问题，用学习残差F(x)&#x3D;0更新该冗余层的参数来代替学习h(x)&#x3D;x更新冗余层的参数。\nNorm（层归一化，Layer Normalization）用于将每一层的输出进行标准化，保持均值为 0，方差为 1。它有助于加速训练，并且提高模型的稳定性。 使用到的归一化方法是Layer Normalization)\nLN是在同一个样本中不同神经元之间进行归一化，而BN是在同一个batch中不同样本之间的同一位置的神经元之间进行归一化。BN是对于相同的维度进行归一化，但是咱们NLP中输入的都是词向量，一个300维的词向量，单独去分析它的每一维是没有意义地，在每一维上进行归一化也是适合地，因此这里选用的是LN。\n  \n\n\n\n6. 前馈神经网络 Position-wise Feed-Forward Networks（FFN）\n\n在 Transformer 中，前馈网络层（Feed-Forward Network，FFN）的作用可以概括为一句话： “对每个位置的向量进行非线性变换，增加模型的表达能力。\n全连接层是一个两层的神经网络，先线性变换，然后ReLU非线性，再线性变换。这两层网络就是为了将输入的Z映射到更加高维的空间中然后通过非线性函数ReLU进行筛选，筛选完后再变回原来的维度经过6个encoder后输入到decoder中。\n7. 三种注意力对比学习（三种注意力）Masked Attention、Self-Attention 和 Cross-Attention 的本质是一致的，这一点从代码调用可以看出来，三者的区别在于未来掩码的使用和输入数据的来源：\n\nMasked Attention：用于解码过程，通过掩码屏蔽未来的时间步，确保模型只能基于已生成的部分进行预测，论文中解码器部分的第一个 Attention 使用的是 Masked Self-Attention。\n\nSelf-Attention：查询、键和值矩阵来自同一输入序列，模型通过自注意力机制学习输入序列的全局依赖关系。\n\nCross-Attention：查询矩阵来自解码器的输入，而键和值矩阵来自编码器的输出，解码器的第二个 Attention 模块就是 Cross-Attention，用于从编码器输出中获取相关的上下文信息。\n以机器翻译中的中译英任务为例：对于中文句子 “中国的首都是北京”，假设模型已经生成了部分译文  “The capital of China is”，此时需要预测下一个单词。\n在这一阶段，解码器中的交叉注意力机制会使用当前已生成的译文 “The capital of China is” 的编码表示作为查询，并将编码器对输入句子 “中国的首都是北京” 编码表示作为键和值，通过计算查询与键之间的匹配程度，生成相应的注意力权重，以此从值中提取上下文信息，基于这些信息生成下一个可能的单词（token），比如：“Beijing”。\n\n\n需要掩码注意力的原因在解码阶段，每一步生成的词语只依赖于前面已生成的词语。因此，需要通过 掩码（Masking） 来确保每个位置的注意力只关注其前面的位置，而不允许查看未来的位置。\n输入需要编码器的输入的原因解码器需要编码器的输出作为上下文信息来生成最终的目标序列。编码器提供的上下文信息有助于解码器更好地理解输入序列的语义和结构。\n解码器和编码器的输出\n编码器的输出：编码器生成的隐藏状态序列，包含了输入序列的语义信息。这些信息将被解码器用于生成最终的输出。\n解码器的输出：解码器根据编码器的输出和已生成的部分输出，逐步生成目标序列（首先经过一次线性变换（线性变换层是一个简单的全连接神经网络，它可以把解码组件产生的向量投射到一个比它大得多的，被称为对数几率的向量里），然后Softmax得到输出的概率分布（softmax层会把向量变成概率），然后通过词典，输出概率最大的对应的单词作为我们的预测输出。）\n\n8. GPT与 Transformer 的关系GPT，即 Generative Pre-Training Language Model，是由 OpenAI 团队于 2018 年发布的预训练语言模型。虽然学界普遍认可 BERT 作为预训练语言模型时代的代表，但首先明确提出预训练 - 微调思想的模型其实是 GPT。\nGPT 提出了通用预训练的概念，也就是在海量无监督语料上预训练，进而在每个特定任务上进行微调，从而实现这些任务的巨大收益。虽然在发布之初，由于性能略输于不久后发布的 BERT，没能取得轰动性成果，也没能让 GPT 所使用的 Decoder-Only 架构成为学界研究的主流，但 OpenAI 团队坚定地选择了不断扩大预训练数据、增加模型参数，在 GPT 架构上不断优化，最终在 2020 年发布的 GPT-3 成就了 LLM 时代的基础，并以 GPT-3 为基座模型的 ChatGPT 成功打开新时代的大门，成为 LLM 时代的最强竞争者也是目前的最大赢家。\n9. 相关资料Transformer 论文精读\nCSDN博客文章\nTransformer快速入门\n","categories":["transformer"],"tags":["AI"]},{"title":"hexo博客搭建教程","url":"/2025/08/24/hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/","content":"\n初衷：\n我记得在刚上大三那会儿，想把自己学的一些东西记录到微信公众号上。刚开始坚持了几天，但是后来放弃了，因为大部分是Copy，也缺少自己的思考。\n经过最近的经历，我明白了一个道理：记录也是一个输出的过程。先自己理解，再把学习内容记录下来（记录适合自己学习或者找资料的过程），也是一种输出。\n所以，我决定从此开始，记录自己的过程，无论是：\n技术学习 、碎碎念  、成长困惑  、考试学习\n我想说，我可能走得很慢，但是我不会停下来，向内生长，加油!\n\n1. hexo介绍正如hexo的首页所显示的，它是一款非常快速，简介，高效的博客框架平台，我们可以利用hexo快速生成博客网站的模板，然后部署为我们自己的博客网站\n\n\n2. hexo教程下面博客记录完整搭建的教程，比较简易，我就是根据这个教程来搭建的，简短的时间就可以搭建一个属于自己的博客。\n其中主要步骤包括：\n\n软件的安装如git、Nodejs\nhexo部署到Github\nhexo配置（如用户个人信息、头像、主题下载配置分类目录）\n\n配置好hexo以及主题之后就可以开始写个人博客了，常见的命令是 \nhexo  clean             # 清除缓存文件和生成的静态文件hexo  new &quot;文章名称&quot;     # 新建界面hexo  generate          # 生成静态文件 可以简写成hexo g hexo  server            # 启动服务器   可以简写hexo shexo  deploy            # 部署网站，构建在GitHub的服务器中，网页文件将上传到关联的个人仓库 可以简写成hexo d\n\n\n\nHexo+Next部署github搭建个人博客+优化全过程（完整详细版）\n3. hexo中图片上传问题\n平时我们写文章可能会记录相关的图片，我个人也喜欢将别人写得好好理解的图片记录下来，但是在图片处理的过程中遇到一些问题（网上的方法各异，试了很多都不行），找了很多的教程，也是自己没有好好思考问题出在哪里，弄了很长时间，最后发现了相应的问题，利用AI解决好了，仅以此记录我的处理方式，当然啦，可能有的博主的教程是对的，可能是我没有找到问题，没有解决，最后还是借助AI理解和解决。\n\nTypora安装Typora\nTypora破解教程(破解序列号在评论区)\n​       修改Typora 偏好设置  如下图更改，此操作将图片文件保存路径: .&#x2F;${filename} 即保存到与 当前正在编辑的文件名相同的同级文件\n\n\n修改hexo配置修改_config.yml中的post_asset_folder，false 改为 true，这样修改后，每次 hexo new page生成新文章，都会在文章文件同级目录创建一个与文章文件名同名的文件夹，我们就在这里存放此文章的图片\n注：在md文档中存放的路径为文件夹/图片名称  \n之前找到有一种方式自定义安装一个插件，将md文档中的路径进行转换，第一天可以，但是第二天就不可以，找了很多办法，还是借助AI写了一个脚本实现相应的功能\n\n\n\n\n\n\n具体实现以下是在 hexo 中实现：Markdown 写 文件夹/图片名称 也能自动复制到相对应文章资源文件夹并生成 &#123;% asset_img %&#125; 标签。\n安装依赖打开 Hexo 项目根目录，安装 fs-extra：\nnpm install fs-extra\n\n\nfs-extra 支持递归创建文件夹和复制文件，比原生 fs 更方便。\n\n创建hexo脚本\n在 hexo 项目根目录下创建文件夹 scripts（如果没有的话）：\n\n/scripts\n\n   2.在 scripts 下创建 JS 文件，例如：\n/scripts/auto_asset_img.js\n\n  3.将下面代码复制到 auto_asset_img.js 中：\n&#x27;use strict&#x27;;const fs = require(&#x27;fs&#x27;);const path = require(&#x27;path&#x27;);const fse = require(&#x27;fs-extra&#x27;);function ignore(data) &#123;  const ext = path.extname(data.source).toLowerCase();  return ext !== &#x27;.md&#x27;; // 只处理 md 文件&#125;function action(data) &#123;  const postDir = path.dirname(data.full_source); // 文章所在目录  const content = data.content;  const imgRegex = /!\\[(.*?)\\]\\((.+?)\\)/g;  data.content = content.replace(imgRegex, (match, alt, imgPath) =&gt; &#123;    const srcPath = path.resolve(postDir, imgPath); // 原始图片路径    const fileName = path.basename(imgPath);        // 图片文件名    const destDir = postDir;                        // 文章资源文件夹    const destPath = path.join(destDir, fileName);    try &#123;      if (fs.existsSync(srcPath)) &#123;        fse.ensureDirSync(destDir);        fse.copySync(srcPath, destPath); // 复制到文章资源文件夹        console.log(`Copy image: $&#123;srcPath&#125; -&gt; $&#123;destPath&#125;`);      &#125; else &#123;        console.warn(`Image not found: $&#123;srcPath&#125;`);      &#125;    &#125; catch (err) &#123;      console.error(`Error copying image: $&#123;err&#125;`);    &#125;    // 替换为 Hexo asset_img 标签，只保留文件名    return `&#123;% asset_img $&#123;fileName&#125; $&#123;alt&#125; %&#125;`;  &#125;);  return data;&#125;hexo.extend.filter.register(&#x27;before_post_render&#x27;, data =&gt; &#123;  if (!ignore(data)) &#123;    action(data);  &#125;&#125;, 0);\n\nMarkdown 图片写法在 Markdown 里可以写带文件夹的图片：\n![示例](demo/demo.png)![另一张](test/other.png)\n\n\n图片相对路径是相对于 Markdown 文件所在目录。\n\n 核心原理：\n\nHexo 会在 before_post_render 阶段调用脚本，处理 Markdown 内容。\n脚本读取 Markdown 中所有图片路径，把它们复制到文章资源文件夹。\nMarkdown 内的路径被替换成 &#123;% asset_img %&#125; 标签，hexo 生成时会在文章资源文件夹查找图片。\n\n4.Netlify部署Netlify部署方法\n5. 相关资料插件方法 \n\n","categories":["博客搭建教程"],"tags":["博客搭建教程"]}]