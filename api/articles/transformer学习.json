{"title":"transformer学习","uid":"6d545ad0d2ef8e06091f61b2fdd30512","slug":"transformer学习","date":"2025-08-23T07:59:03.000Z","updated":"2025-08-24T01:55:50.869Z","comments":true,"path":"api/articles/transformer学习.json","keywords":"AI、Python","cover":[],"content":"<h1 id=\"Transformer\"><a href=\"#Transformer\" class=\"headerlink\" title=\"Transformer\"></a>Transformer</h1><h2 id=\"1-Transformer-主要结构及对比-RNN\"><a href=\"#1-Transformer-主要结构及对比-RNN\" class=\"headerlink\" title=\"1. Transformer 主要结构及对比 RNN\"></a>1. Transformer 主要结构及对比 RNN</h2><h3 id=\"编码器（Encoder）\"><a href=\"#编码器（Encoder）\" class=\"headerlink\" title=\"编码器（Encoder）\"></a>编码器（Encoder）</h3><p>编码器由多个相同的编码器层堆叠而成，每一层包含以下部分：</p>\n<ul>\n<li><strong>多头自注意力机制（Multi-Head Self-Attention）</strong>：通过多个注意力头从不同角度学习词与词之间的关系。</li>\n<li><strong>前馈神经网络（Feed-Forward Neural Networks）</strong>：对注意力机制的输出进行非线性变换，增强模型的表达能力。</li>\n<li><strong>残差连接和层归一化（Residual Connection &amp; Layer Normalization）</strong>：帮助信息流动并稳定训练过程，防止梯度消失。</li>\n</ul>\n<h3 id=\"解码器（Decoder）\"><a href=\"#解码器（Decoder）\" class=\"headerlink\" title=\"解码器（Decoder）\"></a>解码器（Decoder）</h3><p>解码器与编码器类似，但每层解码器额外包含以下机制：</p>\n<ul>\n<li><p><strong>掩蔽多头自注意力机制（Masked Multi-Head Attention）</strong>：用于处理目标序列，通过掩码防止当前位置关注未来位置，确保生成过程的自回归特性。</p>\n</li>\n<li><p><strong>编码器 - 解码器注意力机制（Encoder-Decoder Attention）</strong>：使解码器能够关注编码器输出的上下文信息，建立输入与输出序列之间的关联。</p>\n</li>\n<li><p><strong>前馈神经网络（Feed-Forward Neural Network）</strong>：对注意力机制的输出进行非线性变换，增强解码器的表达能力。</p>\n<img src=\"/post/transformer%E5%AD%A6%E4%B9%A0/1755929954799-0892ffcf-1ef2-40dd-a700-1f568f6bbf8d.png\" class=\"\">\n\n<img src=\"/post/transformer%E5%AD%A6%E4%B9%A0/image-20250823160154806.png\" class=\"\" title=\"image-20250823160154806\"></li>\n</ul>\n<h3 id=\"对比-RNN\"><a href=\"#对比-RNN\" class=\"headerlink\" title=\"对比 RNN\"></a>对比 RNN</h3><ul>\n<li><strong>优点</strong>：<ol>\n<li><strong>并行化</strong>：取消递归结构，Transformer 允许序列中的所有位置同时处理，而 RNN 是逐步处理的，这使得 Transformer 能在训练过程中实现并行化，从而大幅加速训练。</li>\n<li><strong>长距离依赖</strong>：RNN 在处理长序列时，容易出现梯度消失或梯度爆炸的问题，而 Transformer 通过自注意力机制能够直接捕捉到序列中任意位置的依赖关系。（引入位置编码， 在不依赖 RNN 结构的情况下，通过位置编码为序列中的每个元素嵌入位置信息，从而使模型能够感知输入的顺序）</li>\n<li><strong>计算效率</strong>：Transformer 在每一层的计算是对称的，计算效率较高，且可以扩展到大规模的训练数据。</li>\n</ol>\n</li>\n<li><strong>RNN 的缺点</strong>：<ol>\n<li><strong>训练速度慢</strong>：由于其顺序处理的特点，RNN 的计算速度较慢，无法进行高效并行。</li>\n<li><strong>长程依赖问题</strong>：RNN 在捕捉长期依赖时表现较差，通常会遭遇梯度消失或爆炸问题，导致学习效果不佳。</li>\n</ol>\n</li>\n</ul>\n<h2 id=\"2-为什么要进行位置嵌入（Positional-Encoding）？\"><a href=\"#2-为什么要进行位置嵌入（Positional-Encoding）？\" class=\"headerlink\" title=\"2. 为什么要进行位置嵌入（Positional Encoding）？\"></a>2. 为什么要进行位置嵌入（Positional Encoding）？</h2><h3 id=\"位置嵌入的作用\"><a href=\"#位置嵌入的作用\" class=\"headerlink\" title=\"位置嵌入的作用\"></a>位置嵌入的作用</h3><p>Transformer 不像 RNN 那样天然具有序列顺序的处理能力，因此需要通过位置嵌入来为模型提供位置信息，使其能理解输入数据的顺序。</p>\n<h3 id=\"位置嵌入的方式\"><a href=\"#位置嵌入的方式\" class=\"headerlink\" title=\"位置嵌入的方式\"></a>位置嵌入的方式</h3><p>最常用的方式是通过 <strong>正弦和余弦函数</strong> 来生成位置嵌入。具体做法是为每个位置计算一个向量，使用不同频率的正弦和余弦函数来表示不同位置的信息。这个方式的优点是它可以在不依赖训练的情况下生成，且具有很好的可扩展性。</p>\n<p>transformer 的自注意力机制（Self-Attention）是位置无关（position-agnostic）的。也就是说，如果不做任何处理，模型无法区分 “我爱你” 和 “你爱我” 这两个句子的差异，因为自注意力机制只关注 token 之间的相关性，而不考虑它们在序列中的顺序。</p>\n<p>为了让模型感知到 token 的位置信息，Transformer 引入了位置编码。</p>\n<p>在原始论文中，Transformer 使用的是固定位置编码（Positional Encoding），其公式如下：</p>\n<img src=\"/post/transformer%E5%AD%A6%E4%B9%A0/image-20250823160208830.png\" class=\"\" title=\"image-20250823160208830\">\n\n<p>其中：</p>\n<p>pos 表示位置索引（Position）。</p>\n<p>i 表示维度索引。</p>\n<p>dmodel 是嵌入向量的维度。</p>\n<p>流程：输入的是一个整数索引（位置序号 0,1,2,…）。位置编码模块先把这些整数映射成与词向量同维度的向量（例如 512 维），再把结果加到词向量上。</p>\n<p><strong>如何理解位置嵌入</strong></p>\n<p><a href=\"https://www.zhihu.com/question/347678607\"><strong>https://www.zhihu.com/question/347678607</strong></a></p>\n<h2 id=\"3-自注意力机制\"><a href=\"#3-自注意力机制\" class=\"headerlink\" title=\"3. 自注意力机制\"></a>3. 自注意力机制</h2><h3 id=\"自注意力机制的作用\"><a href=\"#自注意力机制的作用\" class=\"headerlink\" title=\"自注意力机制的作用\"></a>自注意力机制的作用</h3><ul>\n<li>随着模型处理输入序列的每个单词，自注意力会关注整个输入序列的所有单词，帮助模型对本单词更好地进行编码。在处理过程中，自注意力机制会将对所有相关单词的理解融入到我们正在处理的单词中。更具体的功能如下：</li>\n<li>序列建模：自注意力可以用于序列数据（例如文本、时间序列、音频等）的建模。它可以捕捉序列中不同位置的依赖关系，从而更好地理解上下文。这对于机器翻译、文本生成、情感分析等任务非常有用。</li>\n<li>并行计算：自注意力可以并行计算，这意味着可以有效地在现代硬件上进行加速。相比于RNN和CNN等序列模型，它更容易在GPU和TPU等硬件上进行高效的训练和推理。（因为在自注意力中可以并行的计算得分）</li>\n<li>长距离依赖捕捉：传统的循环神经网络（RNN）在处理长序列时可能面临梯度消失或梯度爆炸的问题。自注意力可以更好地处理长距离依赖关系，因为它不需要按顺序处理输入序列。</li>\n</ul>\n<h3 id=\"Q-K-V-的含义\"><a href=\"#Q-K-V-的含义\" class=\"headerlink\" title=\"Q, K, V 的含义\"></a>Q, K, V 的含义</h3><ul>\n<li><strong>Q（Query）</strong>：查询向量，表示当前关注的词。</li>\n<li><strong>K（Key）</strong>：键向量，表示其它词的特征。</li>\n<li><strong>V（Value）</strong>：值向量，表示与键向量关联的实际信息。</li>\n</ul>\n<h3 id=\"注意力分数计算公式\"><a href=\"#注意力分数计算公式\" class=\"headerlink\" title=\"注意力分数计算公式\"></a>注意力分数计算公式</h3><img src=\"/post/transformer%E5%AD%A6%E4%B9%A0/1755932645228-418ec5d4-1dd9-4c70-9f88-58953a30032a.png\" class=\"\">\n\n\n\n<p>注意力分数的计算方式是通过 <strong>Q 和 K 的点积</strong> 来衡量 Query 与 Key 之间的相关性。计算公式为：其中，<code>d_k</code> 是 K 向量的维度，<code>*</code> 表示矩阵乘法，<code>softmax</code> 用于标准化分数，使其成为概率分布。</p>\n<p><strong>问题：为什么要</strong><strong><font style=\"color:rgb(77, 77, 77);\">除以dk？</font></strong></p>\n<p>当 dk 较大时，点积的数值可能会过大，导致 Softmax 过后的梯度变得极小</p>\n<img src=\"/post/transformer%E5%AD%A6%E4%B9%A0/1755932140113-a4ff653f-5e50-4ed2-abdc-fd1cc7d2f800.png\" class=\"\">\n\n\n\n<img src=\"/post/transformer%E5%AD%A6%E4%B9%A0/image-20250823160232846.png\" class=\"\" title=\"image-20250823160232846\">\n\n\n\n\n\n<h3 id=\"Softmax函数\"><a href=\"#Softmax函数\" class=\"headerlink\" title=\"Softmax函数\"></a><strong><font style=\"color:rgb(77, 77, 77);\">Softmax函数</font></strong></h3><img src=\"/post/transformer%E5%AD%A6%E4%B9%A0/image-20250823160242032.png\" class=\"\" title=\"image-20250823160242032\">\n\n<p>在 Transformer 模型中，Softmax 函数不仅在计算注意力权重时用到，在预测阶段的输出处理环节也会用到，因为预测 token 的过程可以看成是多分类问题。</p>\n<p><font style=\"color:rgb(77, 77, 77);\">Softmax或称归一化指数函数，它将每一个元素的范围都压缩到（0，1）之间，并且所有元素的和为1</font></p>\n<p><font style=\"color:rgb(77, 77, 77);\">最后经过线性层后进入softmax函数，将每个值向量乘以softmax分数(这是为了准备之后将它们求和)。这里的直觉是希望关注语义上相关的单词，并弱化不相关的单词(例如，让它们乘以0.001这样的小数)。</font></p>\n<p><font style=\"color:rgb(77, 77, 77);\">Softmax 通过指数变换放大数值间的差异，让较大的值对应更高的概率，同时避免了负值和数值过小的问题，让模型聚焦于权重最高的位置，同时保留全局信息（低权重仍非零）</font></p>\n<h2 id=\"4-多头注意力机制\"><a href=\"#4-多头注意力机制\" class=\"headerlink\" title=\"4. 多头注意力机制\"></a>4. 多头注意力机制</h2><p>多头注意力机制就是存在多个不同的权重矩阵（Q、K、V），形成多个矩阵 Z，再把它们 按最后一维（hidden）拼接（concat）→ 做一次线性变换 得到最终输出。</p>\n<p>线性层把拼接后的多头结果 Z_concat（形状 batch×seq×d_model）重新线性映射回与输入相同的维度，同时让网络可以学习如何融合不同头的信息。</p>\n<img src=\"/post/transformer%E5%AD%A6%E4%B9%A0/image-20250823160250773.png\" class=\"\" title=\"image-20250823160250773\">\n\n\n\n<img src=\"/post/transformer%E5%AD%A6%E4%B9%A0/image-20250823160258727.png\" class=\"\" title=\"image-20250823160258727\">\n\n<img src=\"/post/transformer%E5%AD%A6%E4%B9%A0/image-20250823160307258.png\" class=\"\" title=\"image-20250823160307258\">\n\n\n\n<h2 id=\"5-残差层和归一化层的作用\"><a href=\"#5-残差层和归一化层的作用\" class=\"headerlink\" title=\"5. 残差层和归一化层的作用\"></a>5. 残差层和归一化层的作用</h2><h3 id=\"Add（残差连接，Residual-Connection）\"><a href=\"#Add（残差连接，Residual-Connection）\" class=\"headerlink\" title=\"Add（残差连接，Residual Connection）\"></a>Add（残差连接，Residual Connection）</h3><p>残差连接是一种跳跃连接（Skip Connection），它将层的输入直接加到输出上（观察架构图中的箭头）：</p>\n<p>Add，就是在z的基础上加了一个残差块X，加入残差块的目的是为了防止在深度神经网络的训练过程中发生退化的问题，退化的意思就是深度神经网络通过增加网络的层数，Loss逐渐减小，然后趋于稳定达到饱和，然后再继续增加网络层数，Loss反而增大</p>\n<img src=\"/post/transformer%E5%AD%A6%E4%B9%A0/QQ_1756000504529.png\" class=\"\" title=\"img\">\n\n<p>通过直接将输入添加到输出中，帮助缓解深度网络中的梯度消失问题，保证信息能够有效流动。这种连接方式有效缓解了深层神经网络的梯度消失问题</p>\n<h3 id=\"ResNet残差神经网络\"><a href=\"#ResNet残差神经网络\" class=\"headerlink\" title=\"ResNet残差神经网络\"></a>ResNet残差神经网络</h3><p>为了了解残差块，我们引入ResNet残差神经网络，神经网络退化指的是在达到最优网络层数之后，神经网络还在继续训练导致Loss增大，对于多余的层，我们需要保证多出来的网络进行恒等映射。只有进行了恒等映射之后才能保证这多出来的神经网络不会影响到模型的效果。残差连接主要是为了防止网络退化。</p>\n<img src=\"/post/transformer%E5%AD%A6%E4%B9%A0/QQ_1756000520763.png\" class=\"\" title=\"img\">\n\n<p>上图就是构造的一个残差块，X是输入值，F（X）是经过第一层线性变换后并且激活的输出，在第二层线性变化之后，激活之前，F（X）加入了这一层输入值X，然后再进行激活后输出。</p>\n<p>要恒等映射，我们只需要让F（X）&#x3D;0就可以了。x经过线性变换（随机初始化权重一般偏向于0），输出值明显会偏向于0，而且经过激活函数Relu会将负数变为0，过滤了负数的影响。<br>这样当网络自己决定哪些网络层为冗余层时，使用ResNet的网络很大程度上解决了学习恒等映射的问题，用学习残差F(x)&#x3D;0更新该冗余层的参数来代替学习h(x)&#x3D;x更新冗余层的参数。</p>\n<h3 id=\"Norm（层归一化，Layer-Normalization）\"><a href=\"#Norm（层归一化，Layer-Normalization）\" class=\"headerlink\" title=\"Norm（层归一化，Layer Normalization）\"></a>Norm（层归一化，Layer Normalization）</h3><p>用于将每一层的输出进行标准化，保持均值为 0，方差为 1。它有助于加速训练，并且提高模型的稳定性。 使用到的归一化方法是Layer Normalization)</p>\n<p>LN是在同一个样本中不同神经元之间进行归一化，而BN是在同一个batch中不同样本之间的同一位置的神经元之间进行归一化。<br>BN是对于相同的维度进行归一化，但是咱们NLP中输入的都是词向量，一个300维的词向量，单独去分析它的每一维是没有意义地，在每一维上进行归一化也是适合地，因此这里选用的是LN。</p>\n<img src=\"/post/transformer%E5%AD%A6%E4%B9%A0/image-20250823160320441.png\" class=\"\" title=\"image-20250823160320441\">  \n\n<img src=\"/post/transformer%E5%AD%A6%E4%B9%A0/image-20250823160328137.png\" class=\"\" title=\"image-20250823160328137\">\n\n<h2 id=\"6-前馈神经网络-Position-wise-Feed-Forward-Networks（FFN）\"><a href=\"#6-前馈神经网络-Position-wise-Feed-Forward-Networks（FFN）\" class=\"headerlink\" title=\"6. 前馈神经网络 Position-wise Feed-Forward Networks（FFN）\"></a>6. <font style=\"color:rgb(85, 85, 85);\">前馈神经网络 Position-wise Feed-Forward Networks（FFN）</font></h2><img src=\"/post/transformer%E5%AD%A6%E4%B9%A0/image-20250823160337281.png\" class=\"\" title=\"image-20250823160337281\">\n\n<p>在 Transformer 中，前馈网络层（Feed-Forward Network，FFN）的作用可以概括为一句话： “对每个位置的向量进行非线性变换，增加模型的表达能力。</p>\n<p>全连接层是一个两层的神经网络，先线性变换，然后ReLU非线性，再线性变换。这两层网络就是为了将输入的Z映射到更加高维的空间中然后通过非线性函数ReLU进行筛选，筛选完后再变回原来的维度经过6个encoder后输入到decoder中。</p>\n<h2 id=\"7-三种注意力\"><a href=\"#7-三种注意力\" class=\"headerlink\" title=\"7. 三种注意力\"></a>7. 三种注意力</h2><h3 id=\"对比学习（三种注意力）\"><a href=\"#对比学习（三种注意力）\" class=\"headerlink\" title=\"对比学习（三种注意力）\"></a>对比学习（三种注意力）</h3><p>Masked Attention、Self-Attention 和 Cross-Attention 的本质是一致的，这一点从代码调用可以看出来，三者的区别在于未来掩码的使用和输入数据的来源：</p>\n<ul>\n<li><p><strong>Masked Attention</strong>：用于解码过程，通过掩码屏蔽未来的时间步，确保模型只能基于已生成的部分进行预测，论文中解码器部分的第一个 Attention 使用的是 Masked Self-Attention。</p>\n</li>\n<li><p><strong>Self-Attention</strong>：查询、键和值矩阵来自同一输入序列，模型通过自注意力机制学习输入序列的全局依赖关系。</p>\n</li>\n<li><p><strong>Cross-Attention</strong>：查询矩阵来自解码器的输入，而键和值矩阵来自编码器的输出，解码器的第二个 Attention 模块就是 Cross-Attention，用于从编码器输出中获取相关的上下文信息。</p>\n<p>以机器翻译中的中译英任务为例：对于中文句子 “中国的首都是北京”，假设模型已经生成了部分译文  “The capital of China is”，此时需要预测下一个单词。</p>\n<p>在这一阶段，解码器中的交叉注意力机制会使用当前已生成的译文 “The capital of China is” 的编码表示作为查询，并将编码器对输入句子 “中国的首都是北京” 编码表示作为键和值，通过计算查询与键之间的匹配程度，生成相应的注意力权重，以此从值中提取上下文信息，基于这些信息生成下一个可能的单词（token），比如：“Beijing”。</p>\n</li>\n</ul>\n<h3 id=\"需要掩码注意力的原因\"><a href=\"#需要掩码注意力的原因\" class=\"headerlink\" title=\"需要掩码注意力的原因\"></a>需要掩码注意力的原因</h3><p>在解码阶段，每一步生成的词语只依赖于前面已生成的词语。因此，需要通过 <strong>掩码（Masking）</strong> 来确保每个位置的注意力只关注其前面的位置，而不允许查看未来的位置。</p>\n<h3 id=\"输入需要编码器的输入的原因\"><a href=\"#输入需要编码器的输入的原因\" class=\"headerlink\" title=\"输入需要编码器的输入的原因\"></a>输入需要编码器的输入的原因</h3><p>解码器需要编码器的输出作为上下文信息来生成最终的目标序列。编码器提供的上下文信息有助于解码器更好地理解输入序列的语义和结构。</p>\n<h3 id=\"解码器和编码器的输出\"><a href=\"#解码器和编码器的输出\" class=\"headerlink\" title=\"解码器和编码器的输出\"></a>解码器和编码器的输出</h3><ul>\n<li><strong>编码器的输出</strong>：编码器生成的隐藏状态序列，包含了输入序列的语义信息。这些信息将被解码器用于生成最终的输出。</li>\n<li><strong>解码器的输出</strong>：解码器根据编码器的输出和已生成的部分输出，逐步生成目标序列（首先经过一次线性变换（线性变换层是一个简单的全连接神经网络，它可以把解码组件产生的向量投射到一个比它大得多的，被称为对数几率的向量里），然后Softmax得到输出的概率分布（softmax层会把向量变成概率），然后通过词典，输出概率最大的对应的单词作为我们的预测输出。）</li>\n</ul>\n<h2 id=\"8-GPT与-Transformer-的关系\"><a href=\"#8-GPT与-Transformer-的关系\" class=\"headerlink\" title=\"8. GPT与 Transformer 的关系\"></a>8. GPT与 Transformer 的关系</h2><p>GPT，即 Generative Pre-Training Language Model，是由 OpenAI 团队于 2018 年发布的预训练语言模型。虽然学界普遍认可 BERT 作为预训练语言模型时代的代表，但首先明确提出预训练 - 微调思想的模型其实是 GPT。</p>\n<p>GPT 提出了通用预训练的概念，也就是在海量无监督语料上预训练，进而在每个特定任务上进行微调，从而实现这些任务的巨大收益。虽然在发布之初，由于性能略输于不久后发布的 BERT，没能取得轰动性成果，也没能让 GPT 所使用的 Decoder-Only 架构成为学界研究的主流，但 OpenAI 团队坚定地选择了不断扩大预训练数据、增加模型参数，在 GPT 架构上不断优化，最终在 2020 年发布的 GPT-3 成就了 LLM 时代的基础，并以 GPT-3 为基座模型的 ChatGPT 成功打开新时代的大门，成为 LLM 时代的最强竞争者也是目前的最大赢家。</p>\n<h2 id=\"9-相关资料\"><a href=\"#9-相关资料\" class=\"headerlink\" title=\"9. 相关资料\"></a>9. 相关资料</h2><p><a href=\"https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/PaperNotes/Transformer%20%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB.md#norm%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96layer-normalization\">Transformer 论文精读</a></p>\n<p><a href=\"https://blog.csdn.net/weixin_42475060/article/details/121101749?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522aa5208d97b0aa71d3e6dfc51465963ca%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=aa5208d97b0aa71d3e6dfc51465963ca&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-121101749-null-null.142%5Ev102%5Epc_search_result_base7&utm_term=transformer&spm=1018.2226.3001.4187\">CSDN博客文章</a></p>\n<p><a href=\"https://zxj-2023.github.io/2025/07/28/%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AE%97%E6%B3%95/Transformer%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99\">Transformer快速入门</a></p>\n","text":"Transformer1. Transformer 主要结构及对比 RNN编码器（Encoder）编码器由多个相同的编码器层堆叠而成，每一层包含以下部分： 多头...","permalink":"/post/transformer学习","photos":[],"count_time":{"symbolsCount":"5.6k","symbolsTime":"5 mins."},"categories":[{"name":"transformer","slug":"transformer","count":1,"path":"api/categories/transformer.json"}],"tags":[{"name":"AI","slug":"AI","count":2,"path":"api/tags/AI.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Transformer\"><span class=\"toc-text\">Transformer</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#1-Transformer-%E4%B8%BB%E8%A6%81%E7%BB%93%E6%9E%84%E5%8F%8A%E5%AF%B9%E6%AF%94-RNN\"><span class=\"toc-text\">1. Transformer 主要结构及对比 RNN</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E7%BC%96%E7%A0%81%E5%99%A8%EF%BC%88Encoder%EF%BC%89\"><span class=\"toc-text\">编码器（Encoder）</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%A7%A3%E7%A0%81%E5%99%A8%EF%BC%88Decoder%EF%BC%89\"><span class=\"toc-text\">解码器（Decoder）</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%AF%B9%E6%AF%94-RNN\"><span class=\"toc-text\">对比 RNN</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#2-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E8%BF%9B%E8%A1%8C%E4%BD%8D%E7%BD%AE%E5%B5%8C%E5%85%A5%EF%BC%88Positional-Encoding%EF%BC%89%EF%BC%9F\"><span class=\"toc-text\">2. 为什么要进行位置嵌入（Positional Encoding）？</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E4%BD%8D%E7%BD%AE%E5%B5%8C%E5%85%A5%E7%9A%84%E4%BD%9C%E7%94%A8\"><span class=\"toc-text\">位置嵌入的作用</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E4%BD%8D%E7%BD%AE%E5%B5%8C%E5%85%A5%E7%9A%84%E6%96%B9%E5%BC%8F\"><span class=\"toc-text\">位置嵌入的方式</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#3-%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6\"><span class=\"toc-text\">3. 自注意力机制</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E4%BD%9C%E7%94%A8\"><span class=\"toc-text\">自注意力机制的作用</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#Q-K-V-%E7%9A%84%E5%90%AB%E4%B9%89\"><span class=\"toc-text\">Q, K, V 的含义</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%88%86%E6%95%B0%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F\"><span class=\"toc-text\">注意力分数计算公式</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#Softmax%E5%87%BD%E6%95%B0\"><span class=\"toc-text\">Softmax函数</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#4-%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6\"><span class=\"toc-text\">4. 多头注意力机制</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#5-%E6%AE%8B%E5%B7%AE%E5%B1%82%E5%92%8C%E5%BD%92%E4%B8%80%E5%8C%96%E5%B1%82%E7%9A%84%E4%BD%9C%E7%94%A8\"><span class=\"toc-text\">5. 残差层和归一化层的作用</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#Add%EF%BC%88%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%EF%BC%8CResidual-Connection%EF%BC%89\"><span class=\"toc-text\">Add（残差连接，Residual Connection）</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#ResNet%E6%AE%8B%E5%B7%AE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C\"><span class=\"toc-text\">ResNet残差神经网络</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#Norm%EF%BC%88%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96%EF%BC%8CLayer-Normalization%EF%BC%89\"><span class=\"toc-text\">Norm（层归一化，Layer Normalization）</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#6-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Position-wise-Feed-Forward-Networks%EF%BC%88FFN%EF%BC%89\"><span class=\"toc-text\">6. 前馈神经网络 Position-wise Feed-Forward Networks（FFN）</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#7-%E4%B8%89%E7%A7%8D%E6%B3%A8%E6%84%8F%E5%8A%9B\"><span class=\"toc-text\">7. 三种注意力</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%E7%A7%8D%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%89\"><span class=\"toc-text\">对比学习（三种注意力）</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E9%9C%80%E8%A6%81%E6%8E%A9%E7%A0%81%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%9A%84%E5%8E%9F%E5%9B%A0\"><span class=\"toc-text\">需要掩码注意力的原因</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%BE%93%E5%85%A5%E9%9C%80%E8%A6%81%E7%BC%96%E7%A0%81%E5%99%A8%E7%9A%84%E8%BE%93%E5%85%A5%E7%9A%84%E5%8E%9F%E5%9B%A0\"><span class=\"toc-text\">输入需要编码器的输入的原因</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%A7%A3%E7%A0%81%E5%99%A8%E5%92%8C%E7%BC%96%E7%A0%81%E5%99%A8%E7%9A%84%E8%BE%93%E5%87%BA\"><span class=\"toc-text\">解码器和编码器的输出</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#8-GPT%E4%B8%8E-Transformer-%E7%9A%84%E5%85%B3%E7%B3%BB\"><span class=\"toc-text\">8. GPT与 Transformer 的关系</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#9-%E7%9B%B8%E5%85%B3%E8%B5%84%E6%96%99\"><span class=\"toc-text\">9. 相关资料</span></a></li></ol></li></ol>","author":{"name":"Aurora","slug":"blog-author","avatar":"/img/logo.jpg","link":"/","description":"","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"hidden":false,"prev_post":{"title":"hexo博客搭建教程","uid":"7a072923df6718781f31165a7507480d","slug":"hexo博客搭建教程","date":"2025-08-24T05:44:09.000Z","updated":"2025-08-28T05:57:42.676Z","comments":true,"path":"api/articles/hexo博客搭建教程.json","keywords":"AI、Python","cover":[],"text":" 初衷： 我记得在刚上大三那会儿，想把自己学的一些东西记录到微信公众号上。刚开始坚持了几天，但是后来放弃了，因为大部分是Copy，也缺少自己的思考。 经过最近的...","permalink":"/post/hexo博客搭建教程","photos":[],"count_time":{"symbolsCount":"3.1k","symbolsTime":"3 mins."},"categories":[{"name":"博客搭建教程","slug":"博客搭建教程","count":1,"path":"api/categories/博客搭建教程.json"}],"tags":[{"name":"博客搭建教程","slug":"博客搭建教程","count":1,"path":"api/tags/博客搭建教程.json"}],"author":{"name":"Aurora","slug":"blog-author","avatar":"/img/logo.jpg","link":"/","description":"","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}},"next_post":{"title":"从当下开始","uid":"cb19e8d0a77d7e0ba3f75625ed3a4645","slug":"从当下开始","date":"2025-08-23T02:51:10.000Z","updated":"2025-08-23T08:36:06.961Z","comments":true,"path":"api/articles/从当下开始.json","keywords":"AI、Python","cover":null,"text":"对学历的理解本硕双非，曾经的理解，双非就像一个标签刻在自己身上，也是会有一种想法，觉得考上双非特别是研究生，是不是落后别人一大截，怎么挺身而出？其实在现在看来，...","permalink":"/post/从当下开始","photos":[],"count_time":{"symbolsCount":778,"symbolsTime":"1 mins."},"categories":[{"name":"碎碎念念","slug":"碎碎念念","count":1,"path":"api/categories/碎碎念念.json"}],"tags":[{"name":"碎碎念念","slug":"碎碎念念","count":1,"path":"api/tags/碎碎念念.json"}],"author":{"name":"Aurora","slug":"blog-author","avatar":"/img/logo.jpg","link":"/","description":"","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}}}